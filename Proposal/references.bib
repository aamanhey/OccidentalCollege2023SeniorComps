@misc{Rabkina2019,
  author = {Rabkina, I. and Forbus, K. D.},
  year = 2019,
  title = {Analogical Reasoning for Intent Recognition and Action Prediction in Multi-Agent Systems},
  publisher = {In Proceedings of the Seventh Annual Conference on Advances in Cognitive Systems},
  city = {Cambridge},
  state = {MA}
}

@misc{Shum2019,
  author = {Shum, M. and Kleiman-Weiner, M. and Littman, M. L. and Tenenbaum, J. B.},
  month = Jul,
  year = 2019,
  title = {Theory of minds: Understanding behavior in groups through inverse planning},
  publisher = {In Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = 33,
  number = 01,

}

@misc{Battalio2001,
  author = {Battalio, R. and Samuelson, L. and Van Huyck, J.},
  year = 2001,
  title = {Optimization incentives and coordination failure in laboratory stag hunt games},
  publisher = {Econometrica}
}

@misc{Ruhl2020,
  author = {Ruhl, Charlotte},
  title = {Theory of Mind},
  year = 2020,
  url = {https://www.simplypsychology.org/theory-of-mind.html}
}

@misc{GymDocs,
  title = {OpenAi Gym Docs},
  url = {https://gym.openai.com/}
}

@article{Brockman2016,
  title={OpenAI gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@article{Volodymyr2013,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Alex Graves and
               Ioannis Antonoglou and
               Daan Wierstra and
               Martin A. Riedmiller},
  title     = {Playing Atari with Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1312.5602},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.5602},
  eprinttype = {arXiv},
  eprint    = {1312.5602},
  timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{Skyrms2001,
  title = {The Stag Hunt},
  author = {Skyrms, Brian},
  month = Mar,
  year = {2001},
  conference = {Pacific Division of the American Philosophical Association},
  url = {https://www.socsci.uci.edu/~bskyrms/bio/papers/StagHunt.pdf}
}

@article{Xiong2018,
  title = {HogRider: Champion Agent of Microsoft Malmo Collaborative AI Challenge},
  volume={32},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/11581},
  abstractNote={ &lt;p&gt; It has been an open challenge for self-interested agents to make optimal sequential decisions in complex multiagent systems, where agents might achieve higher utility via collaboration. The Microsoft Malmo Collaborative AI Challenge (MCAC), which is designed to encourage research relating to various problems in Collaborative AI, takes the form of a Minecraft mini-game where players might work together to catch a pig or deviate from cooperation, for pursuing high scores to win the challenge. Various characteristics, such as complex interactions among agents, uncertainties, sequential decision making and limited learning trials all make it extremely challenging to find effective strategies. We present HogRider---the champion agent of MCAC in 2017 out of 81 teams from 26 countries. One key innovation of HogRider is a generalized agent type hypothesis framework to identify the behavior model of the other agents, which is demonstrated to be robust to observation uncertainty. On top of that, a second key innovation is a novel Q-learning approach to learn effective policies against each type of the collaborating agents. Various ideas are proposed to adapt traditional Q-learning to handle complexities in the challenge, including state-action abstraction to reduce problem scale, a warm start approach using human reasoning for addressing limited learning trials, and an active greedy strategy to balance exploitation-exploration. Challenge results show that HogRider outperforms all the other teams by a significant edge, in terms of both optimality and stability. &lt;/p&gt; },
  number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Xiong, Yanhai and Chen, Haipeng and Zhao, Mengchen and An, Bo},
  year={2018},
  month=Apr
}

@misc{Katja2017,
  title = {The Malmo Collaborative AI Challenge},
  month = Apr,
  year = {2017},
  author = {Katja Hofmann},
  publisher = {Microsoft Research Cambridge},
  url = {https://github.com/Microsoft/malmo-challenge}
}

@misc{Johnson2016,
  author = {Johnson, M. and Hofmann, K. and Hutton, T. and Bignell, D.},
  year = {2016},
  title = {The Malmo Platform for Artificial Intelligence Experimentation},
  publisher = {Proc. 25th International Joint Conference on Artificial Intelligence},
  journal = {AAAI Press},
  city = {Palo Alto},
  state = {California},
  country = {USA}
}

@InCollection{Kuhn2019,
	author       =	{Kuhn, Steven},
	title        =	{{Prisoner’s Dilemma}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/win2019/entries/prisoner-dilemma/}},
	year         =	{2019},
	edition      =	{{W}inter 2019},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@misc{Li2014,
  title = {Strategies in the Stochastic Iterated Prisoner's Dilemma},
  author = {Li, Siweli},
  year = {2014},
  url = {http://math.uchicago.edu/~may/REU2014/REUPapers/Li,Siwei.pdf}
}

@article{Padakandla2021,
  author = {Padakandla, Sindhu},
  title = {A Survey of Reinforcement Learning Algorithms for Dynamically Varying Environments},
  year = {2021},
  issue_date = {July 2022},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {54},
  number = {6},
  issn = {0360-0300},
  url = {https://doi.org/10.1145/3459991},
  doi = {10.1145/3459991},
  abstract = {Reinforcement learning (RL) algorithms find applications in inventory control, recommender systems, vehicular traffic management, cloud computing, and robotics. The real-world complications arising in these domains makes them difficult to solve with the basic assumptions underlying classical RL algorithms. RL agents in these applications often need to react and adapt to changing operating conditions. A significant part of research on single-agent RL techniques focuses on developing algorithms when the underlying assumption of stationary environment model is relaxed. This article provides a survey of RL methods developed for handling dynamically varying environment models. The goal of methods not limited by the stationarity assumption is to help autonomous agents adapt to varying operating conditions. This is possible either by minimizing the rewards lost during learning by RL agent or by finding a suitable policy for the RL agent that leads to efficient operation of the underlying system. A representative collection of these algorithms is discussed in detail in this work along with their categorization and their relative merits and demerits. Additionally, we also review works that are tailored to application domains. Finally, we discuss future enhancements for this field.},
  journal = {ACM Comput. Surv.},
  month = {jul},
  articleno = {127},
  numpages = {25},
  keywords = {sequential decision-making, meta-learning, non-stationary environments, Markov decision processes, Reinforcement learning, regret computation, context detection}
}

@article{Kaelbling1996,
  title={Reinforcement Learning: A Survey},
  author={Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew W.},
  journal={J. Artif. Intell. Res.},
  year={1996},
  volume={4},
  pages={237-285}
}

@article{Arulkumaran2017,
  title={A Brief Survey of Deep Reinforcement Learning},
  author={Kai Arulkumaran and Marc Peter Deisenroth and Miles Brundage and Anil Anthony Bharath},
  journal={ArXiv},
  year={2017},
  volume={abs/1708.05866}
}

@article{Nica2017,
  title={Learning to Maximize Return in a Stag Hunt Collaborative Scenario through Deep Reinforcement Learning},
  author={Nica, Andrei Cristian and Berariu, Tudor and Gogianu, Florin and Florea, Adina Magda},
  journal={2017 19th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)},
  year={2017},
  pages={188-195}
}


@article{Williams1992,
	abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
	author = {Williams, Ronald J. },
	year = {1992},
  month = May,
	date-added = {2022-05-09 02:13:19 -0700},
	date-modified = {2022-05-09 02:13:19 -0700},
	doi = {10.1007/BF00992696},
	id = {Williams1992},
	journal = {Machine Learning},
	number = {3},
	pages = {229--256},
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	url = {https://doi.org/10.1007/BF00992696},
	volume = {8},
	year = {1992},
	bdsk-url-1 = {https://doi.org/10.1007/BF00992696}
}

@misc{Kansal2018,
	title = {Reinforcement Q-Learning from Scratch in Python with OpenAI Gym},
  author = {Kansal, Satwik and Martin, Brendan},
  year = 2018,
	url = {https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/},
	bdsk-url-1 = {https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/}}

@misc{Taxi-v3,
	title = {Taxi-v3 Documentation},
  author = {OpenAI},
	url = {https://gym.openai.com/envs/Taxi-v3/},
	bdsk-url-1 = {https://gym.openai.com/envs/Taxi-v3/}
}

@misc{Kathuria2021,
  title = {Getting Started With OpenAI Gym{:} Creating Custom Gym Environments},
  author = {Kathuria, Ayoosh},
  year = {2021},
  url = {https://blog.paperspace.com/creating-custom-environments-openai-gym/}
}

@misc{Taxi-v3Source,
  author = {OpenAI},
  year = 2022,
  month = Apr,
  url = {https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py}
}

@article{Mohamed2020,
  author    = {Shakir Mohamed and
               Marie{-}Therese Png and
               William Isaac},
  title     = {Decolonial {AI:} Decolonial Theory as Sociotechnical Foresight in
               Artificial Intelligence},
  journal   = {CoRR},
  volume    = {abs/2007.04068},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.04068},
  eprinttype = {arXiv},
  eprint    = {2007.04068},
  timestamp = {Mon, 20 Jul 2020 14:20:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-04068.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Koenecke2020,
	abstract = {Automated speech recognition (ASR) systems are now used in a variety of applications to convert spoken language to text, from virtual assistants, to closed captioning, to hands-free computing. By analyzing a large corpus of sociolinguistic interviews with white and African American speakers, we demonstrate large racial disparities in the performance of five popular commercial ASR systems. Our results point to hurdles faced by African Americans in using increasingly widespread tools driven by speech recognition technology. More generally, our work illustrates the need to audit emerging machine-learning systems to ensure they are broadly inclusive. Automated speech recognition (ASR) systems, which use sophisticated machine-learning algorithms to convert spoken language to text, have become increasingly widespread, powering popular virtual assistants, facilitating automated closed captioning, and enabling digital dictation platforms for health care. Over the last several years, the quality of these systems has dramatically improved, due both to advances in deep learning and to the collection of large-scale datasets used to train the systems. There is concern, however, that these tools do not work equally well for all subgroups of the population. Here, we examine the ability of five state-of-the-art ASR systems---developed by Amazon, Apple, Google, IBM, and Microsoft---to transcribe structured interviews conducted with 42 white speakers and 73 black speakers. In total, this corpus spans five US cities and consists of 19.8 h of audio matched on the age and gender of the speaker. We found that all five ASR systems exhibited substantial racial disparities, with an average word error rate (WER) of 0.35 for black speakers compared with 0.19 for white speakers. We trace these disparities to the underlying acoustic models used by the ASR systems as the race gap was equally large on a subset of identical phrases spoken by black and white individuals in our corpus. We conclude by proposing strategies---such as using more diverse training datasets that include African American Vernacular English---to reduce these performance differences and ensure speech recognition technology is inclusive.},
	author = {Allison Koenecke and Andrew Nam and Emily Lake and Joe Nudell and Minnie Quartey and Zion Mengesha and Connor Toups and John R. Rickford and Dan Jurafsky and Sharad Goel},
	doi = {10.1073/pnas.1915768117},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1915768117},
	journal = {Proceedings of the National Academy of Sciences},
	number = {14},
	pages = {7684-7689},
	title = {Racial disparities in automated speech recognition},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1915768117},
	volume = {117},
	year = {2020},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.1915768117},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1915768117}
}

@misc{Koenecke2019,
  title = {Voicing Erasures},
  year = 2019,
  time-stamp = {1:26},
  author = {Allison Koenecke and Kimberlè Crenshaw and Megan Smith and Safiya Noble and Ruha Benjamin and Sasha Costanza-Chock},
  url = {https://www.ajl.org/voicing-erasure}
}
